{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Government Measurement Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oxford Covid-19 Government Response Tracker (OxCGRT)\n",
    "oxcgrt_url = 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv'\n",
    "oxcgrt_data = requests.get(oxcgrt_url).content\n",
    "oxcgrt_data = pd.read_csv(io.StringIO(oxcgrt_data.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Cases Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/testing/covid-testing-all-observations.csv'\n",
    "test_data = requests.get(test_url).content\n",
    "test_data = pd.read_csv(io.StringIO(test_data.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirmed Cases Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_url = 'https://datahub.io/core/covid-19/r/time-series-19-covid-combined.csv'\n",
    "confirmed_data = requests.get(confirmed_url).content\n",
    "confirmed_data = pd.read_csv(io.StringIO(confirmed_data.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to different date format adopted by different dataset, we need to unify date format across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxcgrt_data['Date'] = oxcgrt_data['Date'].apply(lambda x: pd.to_datetime(x, format='%Y%m%d'))\n",
    "test_data['Date'] = test_data['Date'].apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d'))\n",
    "confirmed_data['Date'] = confirmed_data['Date'].apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map from measurement name to its abbreviation and vice versa for government measure dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2m = {'C1': 'C1_School closing', 'C2': 'C2_Workplace closing', 'C3': 'C3_Cancel public events', 'C4': 'C4_Restrictions on gatherings', \n",
    "       'C5': 'C5_Close public transport', 'C6': 'C6_Stay at home requirements', 'C7': 'C7_Restrictions on internal movement', \n",
    "       'C8': 'C8_International travel controls', 'H1': 'H1_Public information campaigns'}\n",
    "m2a = {v: k for k, v in a2m.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Government measuers is divided into 18 indicators (C1 ~ C8, E1 ~ E4, H1 ~ H5, and M1), and we focus on C1 to C8 and H1,\n",
    "#### because E1 ~ E4 are fiscal and monetary policies and H2 ~ H5 are testing and contact tracing policies and investment in health system.\n",
    "#### A \"Stringency Index\" is also provided to measure the strigency of government measures.\n",
    "#### More descriptions can be found: https://www.bsg.ox.ac.uk/sites/default/files/2020-05/BSG-WP-2020-032-v5.0_0.pdf,\n",
    "#### and https://www.bsg.ox.ac.uk/sites/default/files/Calculation%20and%20presentation%20of%20the%20Stringency%20Index.pdf\n",
    "#### Note that this dataset has been modified officially on April 30th with note: https://www.bsg.ox.ac.uk/sites/default/files/OxCGRT.%20What%27s%20changed%2024%20April%202020.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select C1 to C8, H1 and Stringency Index\n",
    "dc_measure_data = dict()\n",
    "Measure = namedtuple('Measure', 'C1 C2, C3, C4, C5, C6, C7, C8, H1, stringency')\n",
    "for index, row in oxcgrt_data.iterrows():\n",
    "    value = []\n",
    "    for abbr, measure in a2m.items():\n",
    "        if np.isnan(row[measure]):\n",
    "            value.append(None)\n",
    "        else:\n",
    "            value.append(row[measure])\n",
    "    if np.isnan(row['StringencyIndexForDisplay']):\n",
    "        value.append(None)\n",
    "    else:\n",
    "        value.append(row['StringencyIndexForDisplay'])\n",
    "\n",
    "    key = (row['Date'], row['CountryName'])\n",
    "    value = Measure(*value)\n",
    "    dc_measure_data[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to incomplete reports by different governments, there are some invalid (NAN) values in the govrenment measurement dataset.\n",
    "#### We refill invalid values by the following steps:\n",
    "1. sort dictionary by keys which are tuples: first by country, second by date\n",
    "2. refill None values using the value of the before/after days, as measures won't change significantly\n",
    "3. delete a data sample if we cannot find a valid value in maximum 7 before/after days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_measure_data = {elem[0]: elem[1] for elem in sorted(dc_measure_data.items(), key=lambda x: (x[0][1], x[0][0]))}\n",
    "delete_key = []\n",
    "for key, value in dc_measure_data.items():\n",
    "    for s, v in value._asdict().items():\n",
    "        if v is None:\n",
    "            refill = False\n",
    "            # forward pass\n",
    "            cnt = 1\n",
    "            prev_date = key[0] + pd.DateOffset(-1)\n",
    "            prev_key = (prev_date, key[1])\n",
    "            while cnt < 7:\n",
    "                if dc_measure_data.get(prev_key, None) is not None:\n",
    "                    prev_v = getattr(dc_measure_data.get(prev_key, None), s)\n",
    "                    if prev_v is not None:\n",
    "                        dc_measure_data[key] = dc_measure_data[key]._replace(**{s: prev_v})\n",
    "                        refill = True\n",
    "                        break\n",
    "                    else:\n",
    "                        prev_date = prev_date + pd.DateOffset(-1)\n",
    "                        prev_key = (prev_date, key[1])\n",
    "                        cnt += 1\n",
    "                else:\n",
    "                    prev_date = prev_date + pd.DateOffset(-1)\n",
    "                    prev_key = (prev_date, key[1])\n",
    "                    cnt += 1\n",
    "            if not refill:\n",
    "                cnt = 1\n",
    "                after_date = key[0] + pd.DateOffset(1)\n",
    "                after_key = (after_date, key[1])\n",
    "                while cnt < 7:\n",
    "                    if dc_measure_data.get(after_key, None) is not None:\n",
    "                        after_v = getattr(dc_measure_data.get(after_key, None), s)\n",
    "                        if after_v is not None:\n",
    "                            dc_measure_data[key] = dc_measure_data[key]._replace(**{s: after_v})\n",
    "                            refill = True\n",
    "                            break\n",
    "                        else:\n",
    "                            after_date = after_date + pd.DateOffset(-1)\n",
    "                            after_key = (after_date, key[1])\n",
    "                            cnt += 1\n",
    "                    else:\n",
    "                        after_date = after_date + pd.DateOffset(-1)\n",
    "                        after_key = (after_date, key[1])\n",
    "                        cnt += 1\n",
    "                if not refill:\n",
    "                    delete_key.append(key)\n",
    "                    break\n",
    "\n",
    "dc_measure_data = {k: v for k, v in dc_measure_data.items() if k not in delete_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the test cases dataset, different countries report test numbers under one or more standards, including:\n",
    "#### 'tests performed', 'cases tested', 'people tested', ..., and 'unit unclear'.\n",
    "#### More information can be found: https://ourworldindata.org/covid-testing#our-checklist-for-covid-19-testing-data.\n",
    "#### To keep consistency, we only keep the test numbers under one standard which has the maximum available data samples.\n",
    "#### Note that as long as the standard in any single country is the same along the timeline, the analysis is valid.\n",
    "#### Steps are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: get the number of available data for each country under each standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_tested_dict = dict()     # 'sc' means standard and country\n",
    "for index, row in test_data.iterrows():\n",
    "    entity = row['Entity']\n",
    "    country, standard = entity.split('-')\n",
    "    country = country.strip()\n",
    "    standard = standard.strip()\n",
    "    if country not in sc_tested_dict.keys():\n",
    "        sc_tested_dict[country] = {standard: 1}\n",
    "    else:\n",
    "        if standard not in sc_tested_dict[country]:\n",
    "            sc_tested_dict[country][standard] = 1\n",
    "        else:\n",
    "            sc_tested_dict[country][standard] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: find the standard with the maximum number of availabel data for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_tested_dict = dict()    # 'ssc' means single standard and country\n",
    "for country, value in sc_tested_dict.items():\n",
    "    sorted_value = {k: v for k, v in sorted(value.items(), key=lambda item: -item[1])}\n",
    "    standard = list(sorted_value)[0]\n",
    "    ssc_tested_dict[country] = standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: a). remove data that is not consistent with the selected standard; b). reformat Entity name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test_data.iterrows():\n",
    "    entity = row['Entity']\n",
    "    country, standard = entity.split('-')\n",
    "    country = country.strip()\n",
    "    standard = standard.strip()\n",
    "    if standard != ssc_tested_dict[country]:\n",
    "        test_data.drop(index, inplace=True)\n",
    "    else:\n",
    "        test_data.loc[index, 'Entity'] = country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record the number of daily test cases using the number of cumulative cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_tested_dict = dict()\n",
    "Tested = namedtuple('Tested', 'cumulative daily')\n",
    "for index, row in test_data.iterrows():\n",
    "    key = (row['Date'], row['Entity'])\n",
    "    if not np.isnan(row['Daily change in cumulative total']):\n",
    "        dc_tested_dict[key] = Tested(int(row['Cumulative total']), int(row['Daily change in cumulative total']))\n",
    "    else:\n",
    "        dc_tested_dict[key] = Tested(int(row['Cumulative total']), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the confirmed cases dataset, the number of cummulative confirmed cases in some countries are inconsistent, and we need to:\n",
    "#### 1. Remove invalid (NaN) number of confirmed cases.\n",
    "#### 2. For some countries, combine all province/state statistics into a single country-level statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_confirmed_dict = dict()  # 'dc' means date and country\n",
    "for index, row in confirmed_data.iterrows():\n",
    "    key = (row['Date'], row['Country/Region'])\n",
    "    if not np.isnan(row['Confirmed']):\n",
    "        if key in dc_confirmed_dict.keys():\n",
    "            dc_confirmed_dict[key] = dc_confirmed_dict[key] + int(row['Confirmed'])\n",
    "        else:\n",
    "            dc_confirmed_dict[key] = int(row['Confirmed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute daily confirmed cases using the number of cumulative cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confirmed = namedtuple('Confirmed', 'cumulative daily')\n",
    "for key, value in dc_confirmed_dict.items():\n",
    "    prev_date_key = (key[0] + pd.DateOffset(-1), key[1])\n",
    "    if prev_date_key in dc_confirmed_dict.keys():\n",
    "        if not isinstance(dc_confirmed_dict[prev_date_key], Confirmed):\n",
    "            dc_confirmed_dict[key] = Confirmed(value, value - dc_confirmed_dict[prev_date_key])\n",
    "        else:\n",
    "            dc_confirmed_dict[key] = Confirmed(value, value - dc_confirmed_dict[prev_date_key].cumulative)\n",
    "    else:\n",
    "        dc_confirmed_dict[key] = Confirmed(value, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine data from three datasets together using dictionary structure: key - (date, country), value - (Measure, Confirmed, Tested).\n",
    "#### Because the three datasets are collected by different organizations/groups, we need to filter out incomplete data samples.\n",
    "#### Each item represent a sample for a country in a day which appears in all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_combined_data = dict()   # 'dc' means date and country\n",
    "Combined = namedtuple('Combined', 'measure confirmed tested')\n",
    "for key, value in dc_measure_data.items():\n",
    "    if key in dc_confirmed_dict and key in dc_tested_dict:\n",
    "        dc_combined_data[key] = Combined(value, dc_confirmed_dict[key], dc_tested_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have a look at the final combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 2822\n"
     ]
    }
   ],
   "source": [
    "print('Total number of samples: {}'.format(len(dc_combined_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random sample in the integrated dataset:\n",
      "key:  (Timestamp('2020-04-24 00:00:00'), 'Turkey')\n",
      "value:  Combined(measure=Measure(C1=3.0, C2=2.0, C3=2.0, C4=0.0, C5=2.0, C6=2.0, C7=2.0, C8=4.0, H1=2.0, stringency=80.42), confirmed=Confirmed(cumulative=104912, daily=3122), tested=Tested(cumulative=830257, daily=38351))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "key = random.choice(list(dc_combined_data.keys()))\n",
    "print('A random sample in the integrated dataset:')\n",
    "print('key: ', key)\n",
    "print('value: ', dc_combined_data[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the cleaned and integrated data into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'cleaned_integrated_data.pkl'\n",
    "\n",
    "f = open(filename, 'wb')\n",
    "pickle.dump(dc_combined_data, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to load the cleaned and integrated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 2822\n",
      "A random sample in the integrated dataset:\n",
      "key:  (Timestamp('2020-03-23 00:00:00'), 'Zimbabwe')\n",
      "value:  Combined(measure=Measure(C1=0.0, C2=0.0, C3=2.0, C4=3.0, C5=0.0, C6=1.0, C7=1.0, C8=4.0, H1=1.0, stringency=51.45), confirmed=Confirmed(cumulative=3, daily=0), tested=Tested(cumulative=15, daily=None))\n"
     ]
    }
   ],
   "source": [
    "with open(filename, 'rb') as fp:\n",
    "    loaded = pickle.load(fp)\n",
    "    \n",
    "print('Total number of samples: {}'.format(len(dc_combined_data)))\n",
    "\n",
    "key = random.choice(list(dc_combined_data.keys()))\n",
    "print('A random sample in the integrated dataset:')\n",
    "print('key: ', key)\n",
    "print('value: ', dc_combined_data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
